# Video Search Indexing

# Problem Statement

## Searching and Indexing Video/Audio

Simple media types like text have an excellent metaphor to search and index into. For instance, when you open a pdf/word document or book, you can easily search using a text string or a sentence. This search results in an exact position where that sentence lies in the document. Such searches and indexing mechanisms help with navigating and browsing digital text documents. In a similar manner, automated programs can take text strings/sentences as input and search into a database of documents to find relevant documents (and indexes within the document) where the input string/sentence is present. This is not so straightforward with other media types like video and audio.

There are commercial applications that can search into and “identify” music based on a short sample of audio – eg “Shazam”, where a small 20 second snippet of a song, no matter if it is the intro, verse, or chorus, it will create a fingerprint for the recorded sample, consult the database of preprocessed audio music, and use its music recognition algorithm to tell you exactly which song you are listening to. There are also similar other apps for indexing music given a small 20 second audio or even written or sung lyrics – Genius, Musicxmatch, SoundHound, MusicID, Soly etc. When it comes to similar functionality for videos (with audio), the metaphors for searching and indexing are not that easy. Most of the successful commercial applications are limited to using YouTube (or some similar websites) where, search is accomplished using an input text string which is text matched with metadata strings for all the video to find the match. The metadata strings are either user generated or autogenerated using closed caption or audio-to-text mechanisms. Querying a video with a video is one less explored metaphor, which is the focus of this project.

In this project you will be given a database of videos (with synchronized audio). These videos will be long and will have playtimes upwards of ten mins. The main task of your project is given a video (with synchronized audio) snippet of 20-40 seconds, find the matched video and also compute the indexed start frame of the query video into the matched video. As you may well understand, this is a hard problem to solve in the general sense. But we will make this easier by putting restrictions and narrowing the scope. You may assume:

• All database videos are similar in format (width, height, fps) though exact playtimes may vary. The content of these videos will be discriminatory and different.

• The 20-40 second query video snippet used will also be of the same format as the database videos. Furthermore, the content of the query video will be an “exact” match to some content in a video in the database.

Since the query video (with audio) frame sequence is guaranteed to be an exact match in some database video (with audio), one inefficient way to solve this is a brute force comparison of a moving set of windowed frames in each database video with the window of query frames until a least error match is found. But a media aware process can definitely perform more efficiently. One thought might be to preprocess (in an offline manner) your videos to create a digital signature for each video. At demonstration time, you will similarly process your query video to create a sub-signature. The problem then comes down to finding the best match of the sub signature to a signature – which is essentially a pattern matching problem.

Your project will be called as follows: “MyProject.exe QueryVideo.rgb QueryAudio.wav”

The broad steps for your project include:

Preprocessing to compute digital signatures:
Here you are asked to go through a entire video and create a digital signature. There are many ways to create such signatures based on descriptors which should include (but not limited to):

• Shot boundary details: Shot boundaries where one shot transitions discontinuously and abruptly to a new shot is good key indicator to create matching signatures. Any video structurally consists of shots. Computing shot boundaries can help with a fast indexed search rather than a brute force search. For example, if you detect two shot boundaries with a certain number of frames in between them in your query video, then you can look for hypothesis matches in your database videos that follow the same pattern.
Confirmation can then be made by comparing the video and audio data at the exact frames. You will need to address cases with different numbers of shot boundaries. If there are no shot boundaries detected in the query, then you may need to resolve to other metrics given below to compute a signature.

• Color – For every video you can find a color theme. Eg extracting the most dominant colors per frame as a list of numbers and collating this information.

• Motion – For every video you can compute motion statistics. Every frame can be given a quantitative number depending on how much motion you perceive in that frame.

• Sound – Based on the wave PCM samples, you could compute audio threshold levels or even frequencies that give allow you to compute a quantitative assessment of how much sound/frequency there is in an audio segment.

We leave it to your analysis and experimentation to understand the best way to create digital signatures. You will also need to create a digital signature for your query video, and since they will be shorter than the actual video, let’s call it a digital sub-signature.

Sub signature matching:

Given all the signature, which are ultimately patterns of numbers, create a process to match the sub signature within all the signatures to arrive at the best matching one. The output of this process should point to the correct video and the exact frame offset in the video that corresponds to the first frame of video. You may one or multiple matches depending on thresholds used, which may then be evaluated for confirmation using the actual video and audio data.

Displaying the output:

The output displayed needs to be in a custom video player with basic functionality and support audio/video synchronization. It is expected to default to the first frame of the query video. Basic functionality includes play (from current frame), pause while the video is playing and reset to go to the beginning of the video. You are free to use any UI mechanism of choice, as long as you support the basic requirements.

# Setup

- Preprocess database - generate frame difference hashes (hash-diff-videos)
- Store data in a pickle file

## Solution

- Explointing the following assumptions

1. Video frame size remains consistent between database and query videos
2. Video frame rate remains consistent ()
3. Frames are a perfect match i.e the frames match at a pixel level which means at a bit level they are the same

- Preprocessing

1. Read each frame (RGB format)
2. Obtain difference between two consectuive frames
3. Hash the difference of frames
4. Store in a pickle file

- Matching

1. Load the database and UI
2. In the UI upload the video query (10-20 seconds) clip
3. Read frames of the query clip
4. Obtain the difference of frames
5. Hash the difference of frames
6. Match the hashes from the query to the database to obtain video and frame number
7. Resume playback of video from the said frame number by calculating time from frame number and frame rate

### Contributors

1. [Jugal](https://github.com/jugal13)
2. [Sanjay](https://github.com/SanjayRaghavendra)
3. [Prateeksha](https://github.com/pratheeksha22)
4. [Pooja](https://github.com/Poojaas-33)
